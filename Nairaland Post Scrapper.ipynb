{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean code and Experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import requests\n",
    "from lxml import html\n",
    "import datetime\n",
    "from itertools import chain\n",
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# base URL\n",
    "base_url = 'http://www.nairaland.com'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Flatten List of Lists\n",
    "def flattenLL(listOfLists):\n",
    "    \"Flatten one level of nesting\"\n",
    "    return list(chain.from_iterable(listOfLists))\n",
    "\n",
    "# Get next Word (string) after a target string \n",
    "def nextWord(target, source):\n",
    "    source = source.split()\n",
    "    for i, w in enumerate(source):\n",
    "        if w == target:\n",
    "            return source[i+1]\n",
    "\n",
    "# Get next two words after a target string\n",
    "def next2Words(target, source):\n",
    "    source = source.split()\n",
    "    for i, w in enumerate(source):\n",
    "        if w == target:\n",
    "            nxt2 = print(source[i+1],source[i+2])\n",
    "            return(nxt2)\n",
    "\n",
    "# Get word (string) before a target string        \n",
    "def beforeWord(target, source):\n",
    "    source = source.split()\n",
    "    for i, w in enumerate(source):\n",
    "        if w == target:\n",
    "            return source[i-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Start: Load the Base page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get a htmlElement of a particular link\n",
    "def getPagehtmlElems(link):\n",
    "    response = requests.get(link) #get page data from server, block redirects\n",
    "    sourceCode = response.content #get string of source code from response\n",
    "    htmlElem = html.document_fromstring(sourceCode) #make HTML element object\n",
    "    return(htmlElem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Get Sections Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get sections Links given the base page htmlContent\n",
    "def getSectionsLinks(base_url,base_htmlContent):\n",
    "    section_panel =base_htmlContent.cssselect('[class=\"boards\"]') #panel-board sections of all section element\n",
    "    nlist_sectns_links = [sp.xpath('@href') for sp in section_panel[0].cssselect('a')] # list of list of hyperlinks\n",
    "    # Extract links\n",
    "    list_sub_links = flattenLL(nlist_sectns_links)\n",
    "    #format the link relative to the base URL\n",
    "    sections_links = []\n",
    "    for ref in list_sub_links:\n",
    "        sections_links.append(base_url+ref)\n",
    "    return(sections_links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Iterate the Section by pages and Generate links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Number of pages in a section: given section front page html_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Get Number of pages in a section: given section front page html_content\n",
    "def getNumberOfPages(sctn_frontPage):\n",
    "    s_pgCount = sctn_frontPage.xpath('/html/body/div[1]/p[4]/b[2]')\n",
    "    if s_pgCount != []:\n",
    "        return(s_pgCount[0].text)\n",
    "    else:\n",
    "        s_pgCount = sctn_frontPage.xpath('/html/body/div[1]/p[3]/b[2]')\n",
    "        return(int(s_pgCount[0].text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Construct Section pages URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Construct Section pages URL: Given Section Base Link and Section page count\n",
    "# NB last pages are often skipped (0:length(section_pgsCount)-1)\n",
    "def constructPagesLinks(sctn_baseLink,sctn_pgsCount):\n",
    "    sctns_pagesLinks = [sctn_baseLink+'/'+str(i) for i in range(int(sctn_pgsCount))]\n",
    "    return(sctns_pagesLinks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Iterate  Pages for Post Panel Info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Post Panels from Post Table on the page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get Post Panels\n",
    "def getPostsPanel(pageLink):\n",
    "    # 1. Getting page\n",
    "    page = getPagehtmlElems(pageLink)\n",
    "    # 2. Get post table\n",
    "    post_table = page.xpath('/html/body/div[1]/table[3]')[0] # Target the post table on the page\n",
    "    # 3. Get post Panels\n",
    "    post_panelsList = post_table.cssselect('td')  # Extract post panels\n",
    "    \n",
    "    if len(post_panelsList) > 1:\n",
    "        return(post_panelsList)\n",
    "    elif len(post_panelsList) == 1:\n",
    "        post_table2 = page.xpath('/html/body/div[1]/table[2]')[0] # Target the post table on the page\n",
    "        # 3. Get post Panels\n",
    "        post_panelsList2 = post_table2.cssselect('td')  # Extract post panels\n",
    "        return(post_panelsList2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Extract Info from Post panels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extraction Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get Post title form post panel\n",
    "# breaking the panel elements into section using the css-tag 'b' (-bold)\n",
    "def getPostTitle(postPanel):\n",
    "    post_title = [panelSctns.text_content() for panelSctns in postPanel.cssselect('b')][0]\n",
    "    return(post_title.strip())\n",
    "\n",
    "# Get Post Author\n",
    "def getAuthor(postPanel):\n",
    "    ppTextContent = postPanel.text_content()\n",
    "    author = nextWord('by',ppTextContent)\n",
    "    return(author.replace('.',''))\n",
    "\n",
    "# Get Number of post view\n",
    "def getViewCount(postPanel):\n",
    "    ppTextContent = postPanel.text_content()\n",
    "    vcount = beforeWord('views.',ppTextContent)\n",
    "    if vcount == None:\n",
    "        return(0)\n",
    "    else:\n",
    "        return(vcount)\n",
    "    \n",
    "\n",
    "# Get Post Count\n",
    "def getPostCount(postPanel):\n",
    "    ppTextContent = postPanel.text_content()\n",
    "    pscount = beforeWord('posts',ppTextContent) \n",
    "    if pscount == None:\n",
    "        return(0)\n",
    "    else:\n",
    "        return(pscount)\n",
    "\n",
    "# Get Post time\n",
    "# Using Regular Expression\n",
    "def getTime(postPanel):\n",
    "    ppTextContent = postPanel.text_content() \n",
    "    matcher = re.compile(r\".* (\\d{1,2}:\\d{2}(pm|am))\")\n",
    "    m = matcher.match(ppTextContent)\n",
    "    if m == None:\n",
    "        return('NA')\n",
    "    else:\n",
    "        return(m.group(1))\n",
    "\n",
    "# Get Post Date\n",
    "# Using utility function as\n",
    "def getDate(postPanel,currentDay,currentMonth,currentYear):\n",
    "    ppTextContent = postPanel.text_content()\n",
    "    dateMatcher = re.compile(r\".* (\\d{1,2}:\\d{2}(pm|am))\")\n",
    "    dateContent = dateMatcher.split(ppTextContent)[3]\n",
    "    split = dateContent.split('On')\n",
    "    nsplit = len(split) # Check if postPanel text contain 'On'\n",
    "    if nsplit == 1:  # Contain no Date\n",
    "        noDate = (currentMonth,str(currentDay)+',',currentYear)\n",
    "        return(' '.join(noDate))\n",
    "    elif nsplit == 2: # Contain Date\n",
    "        split2 = split[1].split() #split the part containing date element\n",
    "        nsplit2 = len(split2) # Check if Year is included\n",
    "        if nsplit2 == 3:\n",
    "            date_noYear = (split2[0],split2[1]+',',currentYear)\n",
    "            return(' '.join(date_noYear))\n",
    "        elif nsplit2 == 4:\n",
    "            date_wtYear = (split2[0],split2[1],split2[2])\n",
    "            return(' '.join(date_wtYear))\n",
    "        else:\n",
    "            return('NA')\n",
    "    else:\n",
    "        return('NA')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Extract Given Page Post Panels and populate a local database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Extract Given Page Post Panels and populate a local database\n",
    "def extractPagePostPanels(post_panelsList,currDay,currMnt,currYr):\n",
    "    pgDB_postTitles = []\n",
    "    pgDB_authors = []\n",
    "    pgDB_postCounts = []\n",
    "    pgDB_viewCounts= []\n",
    "    pgDB_times = []\n",
    "    pgDB_dates = []\n",
    "    # Exploring each post panel\n",
    "    if post_panelsList != []:\n",
    "        for i in range(len(post_panelsList)):\n",
    "            postPanel = post_panelsList[i]\n",
    "            if postPanel != []:\n",
    "                pgDB_postTitles.append(getPostTitle(postPanel))\n",
    "                pgDB_authors.append(getAuthor(postPanel))\n",
    "                pgDB_postCounts.append(getPostCount(postPanel))\n",
    "                pgDB_viewCounts.append(getViewCount(postPanel))\n",
    "                pgDB_times.append(getTime(postPanel))\n",
    "                pgDB_dates.append(getDate(postPanel,currDay,currMnt,currYr))\n",
    "    return(pgDB_postTitles,pgDB_authors,pgDB_postCounts,pgDB_viewCounts,pgDB_times,pgDB_dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Compact: Iteration over pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Generalizing\n",
    "def getSectionDB(sectionPagesLinks,currDay,currMnt,currYr):\n",
    "    # 1. Initializing Database \n",
    "    DB_post_title = []\n",
    "    DB_author = []\n",
    "    DB_post_cnt =[]\n",
    "    DB_view_cnt = []\n",
    "    DB_time =[]\n",
    "    DB_date = []\n",
    "    \n",
    "    colNames = ['Post Title', 'Author', 'Post Count', 'View Count', 'Time', 'Date']\n",
    "    \n",
    "    # 2. Iterate over pages\n",
    "    pgsCount = len(sectionPagesLinks) # Total Number of pages \n",
    "    for pc in range(pgsCount):\n",
    "        if pc>0 and pc%200 == 0:\n",
    "            print(\"pausing for 5sec\")\n",
    "            time.sleep(5)\n",
    "            #Print progress per page\n",
    "            print('getting page ',pc+1 ,'/',pgsCount,'\\t')\n",
    "            # A. Iterate over pages\n",
    "            pageLink = sectionPagesLinks[pc]\n",
    "            # B. Get posts panel List\n",
    "            pgPostPanelList = getPostsPanel(pageLink) \n",
    "            # Info about number of post topic on the page (Optional)\n",
    "            #print('Extracting ',len(pgPostPanelList),' topic-postes')\n",
    "    \n",
    "            # C. Extract Given Page Post Panels (get a local DB)\n",
    "            infoExtrct = []\n",
    "            infoExtrct = extractPagePostPanels(pgPostPanelList,\n",
    "                                           currDay,currMnt,currYr)\n",
    "        \n",
    "            # D.  Extend Database per page\n",
    "            DB_post_title.extend(infoExtrct[0])\n",
    "            DB_author.extend(infoExtrct[1])\n",
    "            DB_post_cnt.extend(infoExtrct[2])\n",
    "            DB_view_cnt.extend(infoExtrct[3])\n",
    "            DB_time.extend(infoExtrct[4])\n",
    "            DB_date.extend(infoExtrct[5])\n",
    "            #Reset Extractor Tuple\n",
    "            infoExtrct =[]\n",
    "        else:\n",
    "            #Print progress per page\n",
    "            print('getting page ',pc+1 ,'/',pgsCount,'\\t')\n",
    "    \n",
    "            # A. Iterate over pages\n",
    "            pageLink = sectionPagesLinks[pc]\n",
    "    \n",
    "            # B. Get posts panel List\n",
    "            pgPostPanelList = getPostsPanel(pageLink) \n",
    "            # Info about number of post topic on the page (Optional)\n",
    "            #print('Extracting ',len(pgPostPanelList),' topic-postes')\n",
    "    \n",
    "            # C. Extract Given Page Post Panels (get a local DB)\n",
    "            infoExtrct = []\n",
    "            infoExtrct = extractPagePostPanels(pgPostPanelList,\n",
    "                                           currDay,currMnt,currYr)\n",
    "        \n",
    "            # D.  Extend Database per page\n",
    "            DB_post_title.extend(infoExtrct[0])\n",
    "            DB_author.extend(infoExtrct[1])\n",
    "            DB_post_cnt.extend(infoExtrct[2])\n",
    "            DB_view_cnt.extend(infoExtrct[3])\n",
    "            DB_time.extend(infoExtrct[4])\n",
    "            DB_date.extend(infoExtrct[5])\n",
    "            #Reset Extractor Tuple\n",
    "            infoExtrct =[]\n",
    "            \n",
    "        # 3. Prepare Final Data Frame\n",
    "        DB = pd.DataFrame({ 'Post Title' : DB_post_title,\n",
    "                                'Author' : DB_author,\n",
    "                                'Post Count' : DB_post_cnt,\n",
    "                                'View Count' : DB_view_cnt,\n",
    "                                'Time' : DB_time,\n",
    "                                'Date' : DB_date\n",
    "                              },  columns = colNames)\n",
    "            \n",
    "    return(DB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get Specific Section Pages URLs\n",
    "def getSpecSectionPagesLinks(sctn_baseLink):\n",
    "    # Get Front Page \n",
    "    sctn_frontPage = getPagehtmlElems(sctn_baseLink)\n",
    "    # Get the total number of pages in this section\n",
    "    sctn_pgsCount = getNumberOfPages(sctn_frontPage)\n",
    "    sctn_pgsCount\n",
    "    ## Construct Section pages URL\n",
    "    sctn_pgsLinks = constructPagesLinks(sctn_baseLink,sctn_pgsCount)\n",
    "    return(sctn_pgsLinks) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ScrapperInterface(id,sections_links,currDay, currMnt, currYr):\n",
    "    sctn_baseLink = sections_links[id]\n",
    "    sctn_pgsLinks = getSpecSectionPagesLinks(sctn_baseLink)\n",
    "    sctn_DB = getSectionDB(sctn_pgsLinks, currDay, currMnt, currYr)\n",
    "    return(sctn_DB)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Generalizing\n",
    "def getSectionDB2(sectionPagesLinks,currDay,currMnt,currYr):\n",
    "    # 1. Initializing Database \n",
    "    DB_post_title = []\n",
    "    DB_author = []\n",
    "    DB_post_cnt =[]\n",
    "    DB_view_cnt = []\n",
    "    DB_time =[]\n",
    "    DB_date = []\n",
    "    \n",
    "    colNames = ['Post Title', 'Author', 'Post Count', 'View Count', 'Time', 'Date']\n",
    "    \n",
    "    # 2. Iterate over pages\n",
    "    pgsCount = len(sectionPagesLinks) # Total Number of pages \n",
    "    for pc in range(pgsCount):\n",
    "        if pc>0 and pc%200 == 0:\n",
    "            print(\"pausing for 5sec\")\n",
    "            time.sleep(5)\n",
    "            #Print progress per page\n",
    "            print('getting page ',pc+1 ,'/',pgsCount,'\\t')\n",
    "            # A. Iterate over pages\n",
    "            try:\n",
    "                pageLink = sectionPagesLinks[pc]\n",
    "            except:\n",
    "                print('something is wrong at, '+str(pc))\n",
    "                time.sleep(5)\n",
    "                continue\n",
    "            # B. Get posts panel List\n",
    "            try:\n",
    "                pgPostPanelList = getPostsPanel(pageLink) \n",
    "            except:\n",
    "                print('something is wrong at, '+str(pc))\n",
    "                time.sleep(5)\n",
    "                continue\n",
    "            # Info about number of post topic on the page (Optional)\n",
    "            #print('Extracting ',len(pgPostPanelList),' topic-postes')\n",
    "    \n",
    "            # C. Extract Given Page Post Panels (get a local DB)\n",
    "            try:\n",
    "                infoExtrct = []\n",
    "                infoExtrct = extractPagePostPanels(pgPostPanelList,\n",
    "                                           currDay,currMnt,currYr)\n",
    "            except:\n",
    "                print('something is wrong at, '+str(pc))\n",
    "                time.sleep(5)\n",
    "                continue\n",
    "            # D.  Extend Database per page\n",
    "            DB_post_title.extend(infoExtrct[0])\n",
    "            DB_author.extend(infoExtrct[1])\n",
    "            DB_post_cnt.extend(infoExtrct[2])\n",
    "            DB_view_cnt.extend(infoExtrct[3])\n",
    "            DB_time.extend(infoExtrct[4])\n",
    "            DB_date.extend(infoExtrct[5])\n",
    "            #Reset Extractor Tuple\n",
    "            infoExtrct =[]\n",
    "        else:\n",
    "            #Print progress per page\n",
    "            print('getting page ',pc+1 ,'/',pgsCount,'\\t')\n",
    "    \n",
    "            # A. Iterate over pages\n",
    "            try:\n",
    "                pageLink = sectionPagesLinks[pc]\n",
    "            except:\n",
    "                print('something is wrong at, '+str(pc))\n",
    "                time.sleep(5)\n",
    "                continue\n",
    "    \n",
    "            # B. Get posts panel List\n",
    "            try:\n",
    "                pgPostPanelList = getPostsPanel(pageLink) \n",
    "            except:\n",
    "                print('something is wrong at, '+str(pc))\n",
    "                time.sleep(5)\n",
    "                continue\n",
    "            # Info about number of post topic on the page (Optional)\n",
    "            #print('Extracting ',len(pgPostPanelList),' topic-postes')\n",
    "    \n",
    "            # C. Extract Given Page Post Panels (get a local DB)\n",
    "            try:\n",
    "                infoExtrct = []\n",
    "                infoExtrct = extractPagePostPanels(pgPostPanelList,\n",
    "                                           currDay,currMnt,currYr)\n",
    "            except:\n",
    "                print('something is wrong at, '+str(pc))\n",
    "                time.sleep(5)\n",
    "                continue\n",
    "        \n",
    "            # D.  Extend Database per page\n",
    "            DB_post_title.extend(infoExtrct[0])\n",
    "            DB_author.extend(infoExtrct[1])\n",
    "            DB_post_cnt.extend(infoExtrct[2])\n",
    "            DB_view_cnt.extend(infoExtrct[3])\n",
    "            DB_time.extend(infoExtrct[4])\n",
    "            DB_date.extend(infoExtrct[5])\n",
    "            #Reset Extractor Tuple\n",
    "            infoExtrct =[]\n",
    "            \n",
    "        # 3. Prepare Final Data Frame\n",
    "        DB = pd.DataFrame({ 'Post Title' : DB_post_title,\n",
    "                                'Author' : DB_author,\n",
    "                                'Post Count' : DB_post_cnt,\n",
    "                                'View Count' : DB_view_cnt,\n",
    "                                'Time' : DB_time,\n",
    "                                'Date' : DB_date\n",
    "                              },  columns = colNames)\n",
    "            \n",
    "    return(DB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# More Control\n",
    "def ScrapperInterface2(id,sections_links,currDay, currMnt, currYr,start = None, stop=None):\n",
    "    sctn_baseLink = sections_links[id]\n",
    "    sctn_pgsLinks = getSpecSectionPagesLinks(sctn_baseLink)\n",
    "    if start == None:\n",
    "        sctn_DB = getSectionDB2(sctn_pgsLinks, currDay, currMnt, currYr)\n",
    "        return(sctn_DB)\n",
    "    else:\n",
    "        if stop == None:\n",
    "            #spIndx = len(sctn_pgsLinks)\n",
    "            sctn_DB = getSectionDB2(sctn_pgsLinks[start:len(sctn_pgsLinks)], currDay, currMnt, currYr)\n",
    "            return(sctn_DB)\n",
    "        else:\n",
    "            #stpIndx = stop\n",
    "            sctn_DB = getSectionDB2(sctn_pgsLinks[start:stop], currDay, currMnt, currYr)\n",
    "            return(sctn_DB)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final: Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# base URL\n",
    "base_url = 'http://www.nairaland.com'\n",
    "# load Base Page\n",
    "base_page = getPagehtmlElems(base_url)\n",
    "#get all sections URLs \n",
    "sections_links = getSectionsLinks(base_url,base_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://www.nairaland.com/business'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Specify Section link\n",
    "sections_links[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2552'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pgF = getPagehtmlElems(sections_links[6])\n",
    "getNumberOfPages(pgF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting page  1 / 2552 \t\n",
      "getting page  2 / 2552 \t\n",
      "getting page  3 / 2552 \t\n",
      "getting page  4 / 2552 \t\n",
      "getting page  5 / 2552 \t\n",
      "getting page  6 / 2552 \t\n",
      "getting page  7 / 2552 \t\n",
      "getting page  8 / 2552 \t\n",
      "getting page  9 / 2552 \t\n",
      "getting page  10 / 2552 \t\n",
      "getting page  11 / 2552 \t\n",
      "getting page  12 / 2552 \t\n",
      "getting page  13 / 2552 \t\n",
      "getting page  14 / 2552 \t\n",
      "getting page  15 / 2552 \t\n",
      "getting page  16 / 2552 \t\n",
      "getting page  17 / 2552 \t\n",
      "getting page  18 / 2552 \t\n",
      "getting page  19 / 2552 \t\n",
      "getting page  20 / 2552 \t\n",
      "getting page  21 / 2552 \t\n",
      "getting page  22 / 2552 \t\n",
      "getting page  23 / 2552 \t\n",
      "getting page  24 / 2552 \t\n",
      "getting page  25 / 2552 \t\n",
      "getting page  26 / 2552 \t\n",
      "getting page  27 / 2552 \t\n",
      "getting page  28 / 2552 \t\n",
      "getting page  29 / 2552 \t\n",
      "getting page  30 / 2552 \t\n",
      "getting page  31 / 2552 \t\n",
      "getting page  32 / 2552 \t\n",
      "getting page  33 / 2552 \t\n",
      "getting page  34 / 2552 \t\n",
      "getting page  35 / 2552 \t\n",
      "getting page  36 / 2552 \t\n",
      "getting page  37 / 2552 \t\n",
      "getting page  38 / 2552 \t\n",
      "getting page  39 / 2552 \t\n",
      "getting page  40 / 2552 \t\n",
      "getting page  41 / 2552 \t\n",
      "getting page  42 / 2552 \t\n",
      "getting page  43 / 2552 \t\n",
      "getting page  44 / 2552 \t\n",
      "getting page  45 / 2552 \t\n",
      "getting page  46 / 2552 \t\n",
      "getting page  47 / 2552 \t\n",
      "getting page  48 / 2552 \t\n",
      "getting page  49 / 2552 \t\n",
      "getting page  50 / 2552 \t\n",
      "getting page  51 / 2552 \t\n",
      "getting page  52 / 2552 \t\n",
      "getting page  53 / 2552 \t\n",
      "getting page  54 / 2552 \t\n",
      "getting page  55 / 2552 \t\n",
      "getting page  56 / 2552 \t\n",
      "getting page  57 / 2552 \t\n",
      "getting page  58 / 2552 \t\n",
      "getting page  59 / 2552 \t\n",
      "getting page  60 / 2552 \t\n",
      "getting page  61 / 2552 \t\n",
      "getting page  62 / 2552 \t\n",
      "getting page  63 / 2552 \t\n",
      "getting page  64 / 2552 \t\n",
      "getting page  65 / 2552 \t\n",
      "getting page  66 / 2552 \t\n",
      "getting page  67 / 2552 \t\n",
      "getting page  68 / 2552 \t\n",
      "getting page  69 / 2552 \t\n",
      "getting page  70 / 2552 \t\n",
      "getting page  71 / 2552 \t\n",
      "getting page  72 / 2552 \t\n",
      "getting page  73 / 2552 \t\n",
      "getting page  74 / 2552 \t\n",
      "getting page  75 / 2552 \t\n",
      "getting page  76 / 2552 \t\n",
      "getting page  77 / 2552 \t\n",
      "getting page  78 / 2552 \t\n",
      "getting page  79 / 2552 \t\n",
      "getting page  80 / 2552 \t\n",
      "getting page  81 / 2552 \t\n",
      "getting page  82 / 2552 \t\n",
      "getting page  83 / 2552 \t\n",
      "getting page  84 / 2552 \t\n",
      "getting page  85 / 2552 \t\n",
      "getting page  86 / 2552 \t\n",
      "getting page  87 / 2552 \t\n",
      "getting page  88 / 2552 \t\n",
      "getting page  89 / 2552 \t\n",
      "getting page  90 / 2552 \t\n",
      "getting page  91 / 2552 \t\n",
      "getting page  92 / 2552 \t\n",
      "getting page  93 / 2552 \t\n",
      "getting page  94 / 2552 \t\n",
      "getting page  95 / 2552 \t\n",
      "getting page  96 / 2552 \t\n",
      "getting page  97 / 2552 \t\n",
      "getting page  98 / 2552 \t\n",
      "getting page  99 / 2552 \t\n",
      "getting page  100 / 2552 \t\n",
      "getting page  101 / 2552 \t\n",
      "getting page  102 / 2552 \t\n",
      "getting page  103 / 2552 \t\n",
      "getting page  104 / 2552 \t\n",
      "getting page  105 / 2552 \t\n",
      "getting page  106 / 2552 \t\n",
      "getting page  107 / 2552 \t\n",
      "getting page  108 / 2552 \t\n",
      "getting page  109 / 2552 \t\n",
      "getting page  110 / 2552 \t\n",
      "getting page  111 / 2552 \t\n",
      "getting page  112 / 2552 \t\n"
     ]
    }
   ],
   "source": [
    "# Get Specified Section DB\n",
    "currDay = 13\n",
    "currMnt = 'Apr'\n",
    "currYr = '2018'\n",
    "\n",
    "business_DBP1 = ScrapperInterface2(6,sections_links,\n",
    "                                 currDay, currMnt, currYr,start = 0,stop=3717)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Business_DB = pd.concat([business_DBP1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Post Title</th>\n",
       "      <th>Author</th>\n",
       "      <th>Post Count</th>\n",
       "      <th>View Count</th>\n",
       "      <th>Time</th>\n",
       "      <th>Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Jobs/vacancies Section Chatroom</td>\n",
       "      <td>davide470</td>\n",
       "      <td>46075</td>\n",
       "      <td>1621956</td>\n",
       "      <td>12:54pm</td>\n",
       "      <td>Apr 13, 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How To Identify A Scam Interview Invitation</td>\n",
       "      <td>LaurelP</td>\n",
       "      <td>3963</td>\n",
       "      <td>681612</td>\n",
       "      <td>8:08am</td>\n",
       "      <td>Apr 13, 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Jobs/Vacancies Section Directory!</td>\n",
       "      <td>davide470</td>\n",
       "      <td>0</td>\n",
       "      <td>125628</td>\n",
       "      <td>7:01am</td>\n",
       "      <td>Jan 04, 2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This Tuesday, April 10th At 8pm, Please Join A...</td>\n",
       "      <td>npowerng</td>\n",
       "      <td>23</td>\n",
       "      <td>657</td>\n",
       "      <td>1:22pm</td>\n",
       "      <td>Apr 13, 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Post Abuja Jobs Here</td>\n",
       "      <td>MsSteph</td>\n",
       "      <td>19221</td>\n",
       "      <td>1054710</td>\n",
       "      <td>1:21pm</td>\n",
       "      <td>Apr 13, 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Hospitality Jobs @ It's Finest</td>\n",
       "      <td>wynerz1</td>\n",
       "      <td>4</td>\n",
       "      <td>74</td>\n",
       "      <td>1:20pm</td>\n",
       "      <td>Apr 13, 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>GNLD Happened To Me Today For The First Time</td>\n",
       "      <td>Earthbound</td>\n",
       "      <td>126</td>\n",
       "      <td>11452</td>\n",
       "      <td>1:19pm</td>\n",
       "      <td>Apr 13, 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Urgent Vacancies</td>\n",
       "      <td>Ntukidem</td>\n",
       "      <td>2</td>\n",
       "      <td>105</td>\n",
       "      <td>1:18pm</td>\n",
       "      <td>Apr 13, 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Job Opportunity: Earn Over 450k</td>\n",
       "      <td>SteadyIncome11</td>\n",
       "      <td>11</td>\n",
       "      <td>673</td>\n",
       "      <td>1:11pm</td>\n",
       "      <td>Apr 13, 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2018 Pre-employment Internship Scheme At UAC O...</td>\n",
       "      <td>admart</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>1:08pm</td>\n",
       "      <td>Apr 13, 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Freelancer Who's Available To Proofread An Aca...</td>\n",
       "      <td>victor101</td>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "      <td>1:05pm</td>\n",
       "      <td>Apr 13, 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Rewarding Career Opportunities At Stanbic IBTC...</td>\n",
       "      <td>Naijabash</td>\n",
       "      <td>36</td>\n",
       "      <td>1854</td>\n",
       "      <td>1:02pm</td>\n",
       "      <td>Apr 13, 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Massive Job Recruitment At Meristem Securities...</td>\n",
       "      <td>Naijabash</td>\n",
       "      <td>40</td>\n",
       "      <td>1511</td>\n",
       "      <td>1:01pm</td>\n",
       "      <td>Apr 13, 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Job Opportunities At Genesis Group - APPLY NOW</td>\n",
       "      <td>Naijabash</td>\n",
       "      <td>12</td>\n",
       "      <td>1066</td>\n",
       "      <td>1:00pm</td>\n",
       "      <td>Apr 13, 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>New Job Recruitment At Amaiden Energy Nigeria ...</td>\n",
       "      <td>Naijabash</td>\n",
       "      <td>73</td>\n",
       "      <td>1585</td>\n",
       "      <td>1:00pm</td>\n",
       "      <td>Apr 13, 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Job Recruitment At Sales Force Consulting (8 P...</td>\n",
       "      <td>Naijabash</td>\n",
       "      <td>56</td>\n",
       "      <td>1357</td>\n",
       "      <td>12:59pm</td>\n",
       "      <td>Apr 13, 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Oscar Temple Fresh Job Recruitment (6 Positions)</td>\n",
       "      <td>mars2015</td>\n",
       "      <td>13</td>\n",
       "      <td>411</td>\n",
       "      <td>12:57pm</td>\n",
       "      <td>Apr 13, 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Terragon Group Fresh Job Recruitment (5 Positi...</td>\n",
       "      <td>mars2015</td>\n",
       "      <td>13</td>\n",
       "      <td>539</td>\n",
       "      <td>12:55pm</td>\n",
       "      <td>Apr 13, 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2016 Firstbank Graduate Trainee Programme</td>\n",
       "      <td>mayoor15</td>\n",
       "      <td>5795</td>\n",
       "      <td>533908</td>\n",
       "      <td>12:55pm</td>\n",
       "      <td>Apr 13, 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Comart Invite For Finance Assessment Test</td>\n",
       "      <td>kweku8888</td>\n",
       "      <td>11</td>\n",
       "      <td>228</td>\n",
       "      <td>12:55pm</td>\n",
       "      <td>Apr 13, 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Mathematics Teachers Wanted At Zulpha Academy ...</td>\n",
       "      <td>Ajapson</td>\n",
       "      <td>4</td>\n",
       "      <td>106</td>\n",
       "      <td>12:54pm</td>\n",
       "      <td>Apr 13, 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Saro Africa Online Aptitude Test Invite</td>\n",
       "      <td>sayid</td>\n",
       "      <td>346</td>\n",
       "      <td>21947</td>\n",
       "      <td>12:53pm</td>\n",
       "      <td>Apr 13, 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Massive Recruitment At The U.S. Embassy, April...</td>\n",
       "      <td>mars2015</td>\n",
       "      <td>25</td>\n",
       "      <td>1103</td>\n",
       "      <td>12:52pm</td>\n",
       "      <td>Apr 13, 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Mobil Producing Nigeria Unlimited New Job Oppo...</td>\n",
       "      <td>Naijabash</td>\n",
       "      <td>53</td>\n",
       "      <td>5357</td>\n",
       "      <td>12:51pm</td>\n",
       "      <td>Apr 13, 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Jobs At Ikeja Electricity Distribution Company...</td>\n",
       "      <td>Naijabash</td>\n",
       "      <td>10</td>\n",
       "      <td>467</td>\n",
       "      <td>12:51pm</td>\n",
       "      <td>Apr 13, 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Procter And Gamble Fresh Graduate Sales Intern...</td>\n",
       "      <td>joberplanet</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>12:49pm</td>\n",
       "      <td>Apr 13, 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Rivers State Teachers Recruitment 2012</td>\n",
       "      <td>faithobodogulo</td>\n",
       "      <td>17338</td>\n",
       "      <td>1283234</td>\n",
       "      <td>12:42pm</td>\n",
       "      <td>Apr 13, 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Tailors And Fashion Designers Urgently Needed</td>\n",
       "      <td>negga4al</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>12:41pm</td>\n",
       "      <td>Apr 13, 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Graduate Trainee Job At Ortus Global Africa Li...</td>\n",
       "      <td>mars2015</td>\n",
       "      <td>18</td>\n",
       "      <td>683</td>\n",
       "      <td>12:36pm</td>\n",
       "      <td>Apr 13, 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Rovedana Limited Fresh Job Recruitment (14 Pos...</td>\n",
       "      <td>mars2015</td>\n",
       "      <td>20</td>\n",
       "      <td>673</td>\n",
       "      <td>12:33pm</td>\n",
       "      <td>Apr 13, 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187079</th>\n",
       "      <td>Where Can I Submit My Cv To Qualify For Etb On...</td>\n",
       "      <td>temiboye</td>\n",
       "      <td>2</td>\n",
       "      <td>1121</td>\n",
       "      <td>5:41pm</td>\n",
       "      <td>Mar 13, 2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187080</th>\n",
       "      <td>Vmobile -- Programmers And Oracle Jobs</td>\n",
       "      <td>Viper</td>\n",
       "      <td>2</td>\n",
       "      <td>1195</td>\n",
       "      <td>2:14pm</td>\n",
       "      <td>Mar 10, 2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187081</th>\n",
       "      <td>Vacancies In Saro Agrosciences</td>\n",
       "      <td>Nobody</td>\n",
       "      <td>5</td>\n",
       "      <td>2893</td>\n",
       "      <td>8:53am</td>\n",
       "      <td>Mar 10, 2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187082</th>\n",
       "      <td>Leadway Assurance (marketing Executives)</td>\n",
       "      <td>Nobody</td>\n",
       "      <td>3</td>\n",
       "      <td>1508</td>\n",
       "      <td>5:45pm</td>\n",
       "      <td>Mar 09, 2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187083</th>\n",
       "      <td>Java Web Developer Needed Asap</td>\n",
       "      <td>vanso</td>\n",
       "      <td>0</td>\n",
       "      <td>1051</td>\n",
       "      <td>2:35am</td>\n",
       "      <td>Mar 09, 2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187084</th>\n",
       "      <td>Computer Science Students Need Industrial Trai...</td>\n",
       "      <td>gbolly707</td>\n",
       "      <td>5</td>\n",
       "      <td>3547</td>\n",
       "      <td>1:24pm</td>\n",
       "      <td>Mar 08, 2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187085</th>\n",
       "      <td>Please, Is There Any Bank Recruiting Now?  The...</td>\n",
       "      <td>Ningi</td>\n",
       "      <td>2</td>\n",
       "      <td>1231</td>\n",
       "      <td>1:00pm</td>\n",
       "      <td>Mar 07, 2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187086</th>\n",
       "      <td>International Writing Program-3months Residenc...</td>\n",
       "      <td>wills</td>\n",
       "      <td>0</td>\n",
       "      <td>1724</td>\n",
       "      <td>8:22am</td>\n",
       "      <td>Mar 07, 2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187087</th>\n",
       "      <td>Nokia careers watch</td>\n",
       "      <td>cushman</td>\n",
       "      <td>9</td>\n",
       "      <td>2705</td>\n",
       "      <td>7:02pm</td>\n",
       "      <td>Mar 04, 2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187088</th>\n",
       "      <td>Do U Know That Equatorial Trust Bank Test Is C...</td>\n",
       "      <td>temiboye</td>\n",
       "      <td>0</td>\n",
       "      <td>4697</td>\n",
       "      <td>10:01pm</td>\n",
       "      <td>Mar 02, 2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187089</th>\n",
       "      <td>Any Jobs In Oil/Gas Sector?</td>\n",
       "      <td>waa15</td>\n",
       "      <td>14</td>\n",
       "      <td>5829</td>\n",
       "      <td>3:59pm</td>\n",
       "      <td>Mar 01, 2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187090</th>\n",
       "      <td>: Vacancy United States Mission Nigeria - Mana...</td>\n",
       "      <td>wills</td>\n",
       "      <td>0</td>\n",
       "      <td>2451</td>\n",
       "      <td>9:32am</td>\n",
       "      <td>Feb 28, 2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187091</th>\n",
       "      <td>Do You Need The Services Of An Oracle Dba? The...</td>\n",
       "      <td>neduanaekwe</td>\n",
       "      <td>0</td>\n",
       "      <td>1038</td>\n",
       "      <td>11:49am</td>\n",
       "      <td>Feb 27, 2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187092</th>\n",
       "      <td>Make Quick Change</td>\n",
       "      <td>nikinash</td>\n",
       "      <td>0</td>\n",
       "      <td>962</td>\n",
       "      <td>6:27pm</td>\n",
       "      <td>Feb 24, 2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187093</th>\n",
       "      <td>Sports Company Wants Job Seekers !</td>\n",
       "      <td>pixiraver</td>\n",
       "      <td>0</td>\n",
       "      <td>1059</td>\n",
       "      <td>4:18pm</td>\n",
       "      <td>Feb 18, 2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187094</th>\n",
       "      <td>Vacancy! 500 Online Research Graduate Wanted</td>\n",
       "      <td>abidoajayi</td>\n",
       "      <td>0</td>\n",
       "      <td>1367</td>\n",
       "      <td>4:11pm</td>\n",
       "      <td>Feb 15, 2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187095</th>\n",
       "      <td>Any One In The Aviation Sector?i Need A 2 Mont...</td>\n",
       "      <td>BrownEmma</td>\n",
       "      <td>0</td>\n",
       "      <td>1057</td>\n",
       "      <td>10:49am</td>\n",
       "      <td>Feb 14, 2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187096</th>\n",
       "      <td>Careers In Telecommunication Firm</td>\n",
       "      <td>Nobody</td>\n",
       "      <td>4</td>\n",
       "      <td>2109</td>\n",
       "      <td>10:02am</td>\n",
       "      <td>Feb 13, 2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187097</th>\n",
       "      <td>Computer</td>\n",
       "      <td>Nobody</td>\n",
       "      <td>3</td>\n",
       "      <td>1297</td>\n",
       "      <td>2:07pm</td>\n",
       "      <td>Feb 10, 2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187098</th>\n",
       "      <td>Vmobile Is Recruiting</td>\n",
       "      <td>cushman</td>\n",
       "      <td>2</td>\n",
       "      <td>2515</td>\n",
       "      <td>1:06pm</td>\n",
       "      <td>Feb 10, 2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187099</th>\n",
       "      <td>Gold Lynks</td>\n",
       "      <td>forgiven</td>\n",
       "      <td>0</td>\n",
       "      <td>1331</td>\n",
       "      <td>6:48am</td>\n",
       "      <td>Feb 10, 2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187100</th>\n",
       "      <td>Close this thread</td>\n",
       "      <td>larger20</td>\n",
       "      <td>0</td>\n",
       "      <td>1094</td>\n",
       "      <td>5:46am</td>\n",
       "      <td>Feb 08, 2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187101</th>\n",
       "      <td>Hot Opportunities In The Banking Industry!!!!!</td>\n",
       "      <td>Nobody</td>\n",
       "      <td>0</td>\n",
       "      <td>1748</td>\n",
       "      <td>3:49pm</td>\n",
       "      <td>Feb 07, 2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187102</th>\n",
       "      <td>Recruitment In An International Firm</td>\n",
       "      <td>Nobody</td>\n",
       "      <td>0</td>\n",
       "      <td>1758</td>\n",
       "      <td>3:40pm</td>\n",
       "      <td>Feb 07, 2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187103</th>\n",
       "      <td>Models/ Sales Associate - $15/hr + Health Insu...</td>\n",
       "      <td>Naijalife</td>\n",
       "      <td>2</td>\n",
       "      <td>1466</td>\n",
       "      <td>11:29pm</td>\n",
       "      <td>Feb 01, 2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187104</th>\n",
       "      <td>Manager, Income Budget And Reporting At Vmobile</td>\n",
       "      <td>cushman</td>\n",
       "      <td>0</td>\n",
       "      <td>1410</td>\n",
       "      <td>8:06pm</td>\n",
       "      <td>Jan 30, 2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187105</th>\n",
       "      <td>Post Our C.V. For What Purpose?</td>\n",
       "      <td>Bolaji80</td>\n",
       "      <td>2</td>\n",
       "      <td>1350</td>\n",
       "      <td>11:04am</td>\n",
       "      <td>Jan 24, 2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187106</th>\n",
       "      <td>No More NYSC For Hnd Holders? Help to Confirm!</td>\n",
       "      <td>Hndholder</td>\n",
       "      <td>3</td>\n",
       "      <td>1667</td>\n",
       "      <td>4:18am</td>\n",
       "      <td>Dec 10, 2005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187107</th>\n",
       "      <td>Seeking for Job as Logistics Manager</td>\n",
       "      <td>lizangel</td>\n",
       "      <td>3</td>\n",
       "      <td>1529</td>\n",
       "      <td>12:40pm</td>\n",
       "      <td>Dec 09, 2005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187108</th>\n",
       "      <td>Web Developer Needed</td>\n",
       "      <td>woleraymon</td>\n",
       "      <td>0</td>\n",
       "      <td>1441</td>\n",
       "      <td>4:00pm</td>\n",
       "      <td>Dec 01, 2005</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>187109 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Post Title          Author  \\\n",
       "0                         Jobs/vacancies Section Chatroom       davide470   \n",
       "1             How To Identify A Scam Interview Invitation         LaurelP   \n",
       "2                       Jobs/Vacancies Section Directory!       davide470   \n",
       "3       This Tuesday, April 10th At 8pm, Please Join A...        npowerng   \n",
       "4                                    Post Abuja Jobs Here         MsSteph   \n",
       "5                          Hospitality Jobs @ It's Finest         wynerz1   \n",
       "6            GNLD Happened To Me Today For The First Time      Earthbound   \n",
       "7                                        Urgent Vacancies        Ntukidem   \n",
       "8                         Job Opportunity: Earn Over 450k  SteadyIncome11   \n",
       "9       2018 Pre-employment Internship Scheme At UAC O...          admart   \n",
       "10      Freelancer Who's Available To Proofread An Aca...       victor101   \n",
       "11      Rewarding Career Opportunities At Stanbic IBTC...       Naijabash   \n",
       "12      Massive Job Recruitment At Meristem Securities...       Naijabash   \n",
       "13         Job Opportunities At Genesis Group - APPLY NOW       Naijabash   \n",
       "14      New Job Recruitment At Amaiden Energy Nigeria ...       Naijabash   \n",
       "15      Job Recruitment At Sales Force Consulting (8 P...       Naijabash   \n",
       "16       Oscar Temple Fresh Job Recruitment (6 Positions)        mars2015   \n",
       "17      Terragon Group Fresh Job Recruitment (5 Positi...        mars2015   \n",
       "18              2016 Firstbank Graduate Trainee Programme        mayoor15   \n",
       "19              Comart Invite For Finance Assessment Test       kweku8888   \n",
       "20      Mathematics Teachers Wanted At Zulpha Academy ...         Ajapson   \n",
       "21                Saro Africa Online Aptitude Test Invite           sayid   \n",
       "22      Massive Recruitment At The U.S. Embassy, April...        mars2015   \n",
       "23      Mobil Producing Nigeria Unlimited New Job Oppo...       Naijabash   \n",
       "24      Jobs At Ikeja Electricity Distribution Company...       Naijabash   \n",
       "25      Procter And Gamble Fresh Graduate Sales Intern...     joberplanet   \n",
       "26                 Rivers State Teachers Recruitment 2012  faithobodogulo   \n",
       "27          Tailors And Fashion Designers Urgently Needed        negga4al   \n",
       "28      Graduate Trainee Job At Ortus Global Africa Li...        mars2015   \n",
       "29      Rovedana Limited Fresh Job Recruitment (14 Pos...        mars2015   \n",
       "...                                                   ...             ...   \n",
       "187079  Where Can I Submit My Cv To Qualify For Etb On...        temiboye   \n",
       "187080             Vmobile -- Programmers And Oracle Jobs           Viper   \n",
       "187081                     Vacancies In Saro Agrosciences          Nobody   \n",
       "187082           Leadway Assurance (marketing Executives)          Nobody   \n",
       "187083                     Java Web Developer Needed Asap           vanso   \n",
       "187084  Computer Science Students Need Industrial Trai...       gbolly707   \n",
       "187085  Please, Is There Any Bank Recruiting Now?  The...           Ningi   \n",
       "187086  International Writing Program-3months Residenc...           wills   \n",
       "187087                                Nokia careers watch         cushman   \n",
       "187088  Do U Know That Equatorial Trust Bank Test Is C...        temiboye   \n",
       "187089                        Any Jobs In Oil/Gas Sector?           waa15   \n",
       "187090  : Vacancy United States Mission Nigeria - Mana...           wills   \n",
       "187091  Do You Need The Services Of An Oracle Dba? The...     neduanaekwe   \n",
       "187092                                  Make Quick Change        nikinash   \n",
       "187093                 Sports Company Wants Job Seekers !       pixiraver   \n",
       "187094       Vacancy! 500 Online Research Graduate Wanted      abidoajayi   \n",
       "187095  Any One In The Aviation Sector?i Need A 2 Mont...       BrownEmma   \n",
       "187096                  Careers In Telecommunication Firm          Nobody   \n",
       "187097                                           Computer          Nobody   \n",
       "187098                              Vmobile Is Recruiting         cushman   \n",
       "187099                                         Gold Lynks        forgiven   \n",
       "187100                                  Close this thread        larger20   \n",
       "187101     Hot Opportunities In The Banking Industry!!!!!          Nobody   \n",
       "187102               Recruitment In An International Firm          Nobody   \n",
       "187103  Models/ Sales Associate - $15/hr + Health Insu...       Naijalife   \n",
       "187104    Manager, Income Budget And Reporting At Vmobile         cushman   \n",
       "187105                    Post Our C.V. For What Purpose?        Bolaji80   \n",
       "187106     No More NYSC For Hnd Holders? Help to Confirm!       Hndholder   \n",
       "187107               Seeking for Job as Logistics Manager        lizangel   \n",
       "187108                               Web Developer Needed      woleraymon   \n",
       "\n",
       "       Post Count View Count     Time          Date  \n",
       "0           46075    1621956  12:54pm  Apr 13, 2018  \n",
       "1            3963     681612   8:08am  Apr 13, 2018  \n",
       "2               0     125628   7:01am  Jan 04, 2016  \n",
       "3              23        657   1:22pm  Apr 13, 2018  \n",
       "4           19221    1054710   1:21pm  Apr 13, 2018  \n",
       "5               4         74   1:20pm  Apr 13, 2018  \n",
       "6             126      11452   1:19pm  Apr 13, 2018  \n",
       "7               2        105   1:18pm  Apr 13, 2018  \n",
       "8              11        673   1:11pm  Apr 13, 2018  \n",
       "9               0         13   1:08pm  Apr 13, 2018  \n",
       "10              2         24   1:05pm  Apr 13, 2018  \n",
       "11             36       1854   1:02pm  Apr 13, 2018  \n",
       "12             40       1511   1:01pm  Apr 13, 2018  \n",
       "13             12       1066   1:00pm  Apr 13, 2018  \n",
       "14             73       1585   1:00pm  Apr 13, 2018  \n",
       "15             56       1357  12:59pm  Apr 13, 2018  \n",
       "16             13        411  12:57pm  Apr 13, 2018  \n",
       "17             13        539  12:55pm  Apr 13, 2018  \n",
       "18           5795     533908  12:55pm  Apr 13, 2018  \n",
       "19             11        228  12:55pm  Apr 13, 2018  \n",
       "20              4        106  12:54pm  Apr 13, 2018  \n",
       "21            346      21947  12:53pm  Apr 13, 2018  \n",
       "22             25       1103  12:52pm  Apr 13, 2018  \n",
       "23             53       5357  12:51pm  Apr 13, 2018  \n",
       "24             10        467  12:51pm  Apr 13, 2018  \n",
       "25              0         17  12:49pm  Apr 13, 2018  \n",
       "26          17338    1283234  12:42pm  Apr 13, 2018  \n",
       "27              0         12  12:41pm  Apr 13, 2018  \n",
       "28             18        683  12:36pm  Apr 13, 2018  \n",
       "29             20        673  12:33pm  Apr 13, 2018  \n",
       "...           ...        ...      ...           ...  \n",
       "187079          2       1121   5:41pm  Mar 13, 2006  \n",
       "187080          2       1195   2:14pm  Mar 10, 2006  \n",
       "187081          5       2893   8:53am  Mar 10, 2006  \n",
       "187082          3       1508   5:45pm  Mar 09, 2006  \n",
       "187083          0       1051   2:35am  Mar 09, 2006  \n",
       "187084          5       3547   1:24pm  Mar 08, 2006  \n",
       "187085          2       1231   1:00pm  Mar 07, 2006  \n",
       "187086          0       1724   8:22am  Mar 07, 2006  \n",
       "187087          9       2705   7:02pm  Mar 04, 2006  \n",
       "187088          0       4697  10:01pm  Mar 02, 2006  \n",
       "187089         14       5829   3:59pm  Mar 01, 2006  \n",
       "187090          0       2451   9:32am  Feb 28, 2006  \n",
       "187091          0       1038  11:49am  Feb 27, 2006  \n",
       "187092          0        962   6:27pm  Feb 24, 2006  \n",
       "187093          0       1059   4:18pm  Feb 18, 2006  \n",
       "187094          0       1367   4:11pm  Feb 15, 2006  \n",
       "187095          0       1057  10:49am  Feb 14, 2006  \n",
       "187096          4       2109  10:02am  Feb 13, 2006  \n",
       "187097          3       1297   2:07pm  Feb 10, 2006  \n",
       "187098          2       2515   1:06pm  Feb 10, 2006  \n",
       "187099          0       1331   6:48am  Feb 10, 2006  \n",
       "187100          0       1094   5:46am  Feb 08, 2006  \n",
       "187101          0       1748   3:49pm  Feb 07, 2006  \n",
       "187102          0       1758   3:40pm  Feb 07, 2006  \n",
       "187103          2       1466  11:29pm  Feb 01, 2006  \n",
       "187104          0       1410   8:06pm  Jan 30, 2006  \n",
       "187105          2       1350  11:04am  Jan 24, 2006  \n",
       "187106          3       1667   4:18am  Dec 10, 2005  \n",
       "187107          3       1529  12:40pm  Dec 09, 2005  \n",
       "187108          0       1441   4:00pm  Dec 01, 2005  \n",
       "\n",
       "[187109 rows x 6 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Jobs_DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# DB to CSV\n",
    "Jobs_DB.to_csv('../Data/Jobs DB.csv')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Career_DB.to_csv('../Data/CareerDB.csv')\n",
    "#Politics_DB.to_csv('../Data/PoliticsDB.csv')\n",
    "#artGV_DB.to_csv('../Data/ArtGraphicsVideoDB.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#\n",
    "\n",
    "\n",
    "\n",
    "#INVESTIGATION\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#sctn_baseLink = sections_links[2]\n",
    "# Get Front Page \n",
    "#sctn_frontPage = getPagehtmlElems(sctn_baseLink)\n",
    "# Get the total number of pages in this section\n",
    "#sctn_pgsCount = getNumberOfPages(sctn_frontPage)\n",
    "#sctn_pgsCount\n",
    "## Construct Section pages URL\n",
    "#sctn_pgsLinks = constructPagesLinks(sctn_baseLink,sctn_pgsCount)\n",
    "\n",
    "#sctn_DB = getSectionDB(sctn_pgsLinks, currDay, currMnt, currYr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#plink = sctn_pgsLinks[571]\n",
    "#pg = getPagehtmlElems(plink)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#tbl = pg.xpath('/html/body/div[1]/table[3]')[0]\n",
    "#pnls = tbl.cssselect('td')\n",
    "p_panelsList = getPostsPanel(plink)\n",
    "len(p_panelsList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#[ getTime(pnl) for pnl in p_panelsList[32:35]]\n",
    "\n",
    "#getTime(p_panelsList[34])\n",
    "\n",
    "p_panelsList[34].text\n",
    "#extractPagePostPanels(p_panelsList, currDay, currMnt, currYr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p_panelsList[0].text_content()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#/html/body/div[1]/table[2]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p = 10\n",
    "if p>0 and p%2 == 0:\n",
    "    print('Yrs')\n",
    "else:\n",
    "    print('NO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
